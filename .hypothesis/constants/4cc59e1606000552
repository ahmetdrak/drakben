# file: C:\Users\E-YAZILIM\Desktop\drakben\drakbendosyalar\core\llm\token_counter.py
# hypothesis_version: 6.151.6

[0.0, 0.055, 0.07, 0.14, 0.15, 0.25, 0.28, 0.5, 0.59, 0.6, 0.79, 1.25, 1.3, 1.5, 2.0, 2.5, 3.0, 6.0, 10.0, 15.0, 30.0, 60.0, 75.0, 1024, 8192, 16385, 32768, 128000, 131072, 200000, 1000000, 'cl100k_base', 'claude', 'claude-3-haiku', 'claude-3-opus', 'claude-3-sonnet', 'claude-3.5-sonnet', 'content', 'context_window', 'deepseek', 'deepseek-coder', 'default', 'encoding', 'gemma', 'gpt-3.5-turbo', 'gpt-4', 'gpt-4-turbo', 'gpt-4o', 'gpt-4o-mini', 'input_cost', 'input_tokens', 'llama', 'llama-3.1-70b', 'llama-3.1-8b', 'mistral', 'mistral-7b', 'mistral-large', 'model', 'note', 'o200k_base', 'output_cost', 'output_tokens', 'qwen', 'qwen-2.5', 'role', 'system', 'tiktoken_available', 'total_cost']